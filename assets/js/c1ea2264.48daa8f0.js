"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["26406"],{768477:function(e,n,t){t.r(n),t.d(n,{metadata:()=>r,contentTitle:()=>i,default:()=>h,assets:()=>l,toc:()=>d,frontMatter:()=>s});var r=JSON.parse('{"id":"data-operate/import/import-scenes/external-storage-load","title":"External storage data import","description":"\x3c!--","source":"@site/versioned_docs/version-1.2/data-operate/import/import-scenes/external-storage-load.md","sourceDirName":"data-operate/import/import-scenes","slug":"/data-operate/import/import-scenes/external-storage-load","permalink":"/docs/1.2/data-operate/import/import-scenes/external-storage-load","draft":false,"unlisted":false,"tags":[],"version":"1.2","frontMatter":{"title":"External storage data import","language":"en"},"sidebar":"docs","previous":{"title":"Import local data","permalink":"/docs/1.2/data-operate/import/import-scenes/local-file-load"},"next":{"title":"Kafka Data Subscription","permalink":"/docs/1.2/data-operate/import/import-scenes/kafka-load"}}'),o=t("785893"),a=t("250065");let s={title:"External storage data import",language:"en"},i="External storage data import",l={},d=[{value:"HDFS LOAD",id:"hdfs-load",level:2},{value:"Ready to work",id:"ready-to-work",level:3},{value:"start import",id:"start-import",level:3},{value:"S3 LOAD",id:"s3-load",level:2},{value:"Applicable scenarios",id:"applicable-scenarios",level:3},{value:"Preparing",id:"preparing",level:3},{value:"Start Loading",id:"start-loading",level:3},{value:"FAQ",id:"faq",level:3}];function c(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",version:"version",...(0,a.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"external-storage-data-import",children:"External storage data import"})}),"\n",(0,o.jsx)(n.p,{children:"The following mainly introduces how to import data stored in an external system. For example (HDFS, All object stores that support the S3 protocol)"}),"\n",(0,o.jsx)(n.h2,{id:"hdfs-load",children:"HDFS LOAD"}),"\n",(0,o.jsx)(n.h3,{id:"ready-to-work",children:"Ready to work"}),"\n",(0,o.jsxs)(n.p,{children:["Upload the files to be imported to HDFS. For specific commands, please refer to ",(0,o.jsx)(n.a,{href:"https://hadoop.apache.org/docs/r3.3.2/hadoop-project-dist/hadoop-common/FileSystemShell.html#put",children:"HDFS upload command"})]}),"\n",(0,o.jsx)(n.h3,{id:"start-import",children:"start import"}),"\n",(0,o.jsxs)(n.p,{children:["Hdfs load creates an import statement. The import method is basically the same as ",(0,o.jsx)(n.a,{href:"/docs/1.2/data-operate/import/import-way/broker-load-manual",children:"Broker Load"}),", only need to ",(0,o.jsx)(n.code,{children:"WITH BROKER broker_name () "})," statement with the following"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"  LOAD LABEL db_name.label_name \n  (data_desc, ...)\n  WITH HDFS\n  [PROPERTIES (key1=value1, ... )]\n"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"create a table"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE IF NOT EXISTS load_hdfs_file_test\n(\n    id INT,\n    age TINYINT,\n    name VARCHAR(50)\n)\nunique key(id)\nDISTRIBUTED BY HASH(id) BUCKETS 3;\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Import data Execute the following command to import HDFS files:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL demo.label_20220402\n    (\n    DATA INFILE("hdfs://host:port/tmp/test_hdfs.txt")\n    INTO TABLE `load_hdfs_file_test`\n    COLUMNS TERMINATED BY "\\t"            \n    (id,age,name)\n    )\n    with HDFS (\n    "fs.defaultFS"="hdfs://testFs",\n    "hdfs_user"="user"\n    )\n    PROPERTIES\n    (\n    "timeout"="1200",\n    "max_filter_ratio"="0.1"\n    );\n'})}),"\n",(0,o.jsxs)(n.p,{children:["For parameter introduction, please refer to ",(0,o.jsx)(n.a,{href:"/docs/1.2/data-operate/import/import-way/broker-load-manual",children:"Broker Load"}),", HA cluster creation syntax, view through ",(0,o.jsx)(n.code,{children:"HELP BROKER LOAD"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Check import status"}),"\n",(0,o.jsxs)(n.p,{children:["Broker load is an asynchronous import method. The specific import results can be accessed through ",(0,o.jsx)(n.a,{href:"/docs/1.2/sql-manual/sql-reference/Show-Statements/SHOW-LOAD",children:"SHOW LOAD"})," command to view"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'mysql> show load order by createtime desc limit 1\\G;\n*************************** 1. row ***************************\n         JobId: 41326624\n         Label: broker_load_2022_04_15\n         State: FINISHED\n      Progress: ETL:100%; LOAD:100%\n          Type: BROKER\n       EtlInfo: unselected.rows=0; dpp.abnorm.ALL=0; dpp.norm.ALL=27\n      TaskInfo: cluster:N/A; timeout(s):1200; max_filter_ratio:0.1\n      ErrorMsg: NULL\n    CreateTime: 2022-04-01 18:59:06\n  EtlStartTime: 2022-04-01 18:59:11\n EtlFinishTime: 2022-04-01 18:59:11\n LoadStartTime: 2022-04-01 18:59:11\nLoadFinishTime: 2022-04-01 18:59:11\n           URL: NULL\n    JobDetails: {"Unfinished backends":{"5072bde59b74b65-8d2c0ee5b029adc0":[]},"ScannedRows":27,"TaskNumber":1,"All backends":{"5072bde59b74b65-8d2c0ee5b029adc0":[36728051]},"FileNumber":1,"FileSize":5540}\n1 row in set (0.01 sec)\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"s3-load",children:"S3 LOAD"}),"\n",(0,o.jsx)(n.p,{children:"Starting from version 0.14, Doris supports the direct import of data from online storage systems that support the S3 protocol through the S3 protocol."}),"\n",(0,o.jsx)(n.p,{children:"This document mainly introduces how to import data stored in AWS S3. It also supports the import of other object storage systems that support the S3 protocol."}),"\n",(0,o.jsx)(n.h3,{id:"applicable-scenarios",children:"Applicable scenarios"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Source data in S3 protocol accessible storage systems, such as S3."}),"\n",(0,o.jsx)(n.li,{children:"Data volumes range from tens to hundreds of GB."}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"preparing",children:"Preparing"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Standard AK and SK\nFirst, you need to find or regenerate AWS ",(0,o.jsx)(n.code,{children:"Access keys"}),", you can find the generation method in ",(0,o.jsx)(n.code,{children:"My Security Credentials"})," of AWS console, as shown in the following figure:\n",(0,o.jsx)(n.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:t(120852).Z+"",children:"AK_SK"}),"\nSelect ",(0,o.jsx)(n.code,{children:"Create New Access Key"})," and pay attention to save and generate AK and SK."]}),"\n",(0,o.jsxs)(n.li,{children:["Prepare REGION and ENDPOINT\nREGION can be selected when creating the bucket or can be viewed in the bucket list. ENDPOINT can be found through REGION on the following page ",(0,o.jsx)(n.a,{href:"https://docs.aws.amazon.com/general/latest/gr/s3.html#s3_region",children:"AWS Documentation"})]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Other cloud storage systems can find relevant information compatible with S3 in corresponding documents"}),"\n",(0,o.jsx)(n.h3,{id:"start-loading",children:"Start Loading"}),"\n",(0,o.jsxs)(n.p,{children:["Like ",(0,o.jsx)(n.a,{href:"/docs/1.2/data-operate/import/import-way/broker-load-manual",children:"Broker Load"}),"  just replace ",(0,o.jsx)(n.code,{children:"WITH BROKER broker_name ()"})," with"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'    WITH S3\n    (\n        "AWS_ENDPOINT" = "AWS_ENDPOINT",\n        "AWS_ACCESS_KEY" = "AWS_ACCESS_KEY",\n        "AWS_SECRET_KEY"="AWS_SECRET_KEY",\n        "AWS_REGION" = "AWS_REGION"\n    )\n'})}),"\n",(0,o.jsx)(n.p,{children:"example:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'    LOAD LABEL example_db.example_label_1\n    (\n        DATA INFILE("s3://your_bucket_name/your_file.txt")\n        INTO TABLE load_test\n        COLUMNS TERMINATED BY ","\n    )\n    WITH S3\n    (\n        "AWS_ENDPOINT" = "AWS_ENDPOINT",\n        "AWS_ACCESS_KEY" = "AWS_ACCESS_KEY",\n        "AWS_SECRET_KEY"="AWS_SECRET_KEY",\n        "AWS_REGION" = "AWS_REGION"\n    )\n    PROPERTIES\n    (\n        "timeout" = "3600"\n    );\n'})}),"\n",(0,o.jsx)(n.h3,{id:"faq",children:"FAQ"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["S3 SDK uses virtual-hosted style by default. However, some object storage systems may not be enabled or support virtual-hosted style access. At this time, we can add the ",(0,o.jsx)(n.code,{children:"use_path_style"})," parameter to force the use of path style:"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'   WITH S3\n   (\n         "AWS_ENDPOINT" = "AWS_ENDPOINT",\n         "AWS_ACCESS_KEY" = "AWS_ACCESS_KEY",\n         "AWS_SECRET_KEY"="AWS_SECRET_KEY",\n         "AWS_REGION" = "AWS_REGION",\n         "use_path_style" = "true"\n   )\n'})}),"\n",(0,o.jsxs)(n.version,{since:"1.2",children:["\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Support using temporary security credentials to access object stores that support the S3 protocol:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'  WITH S3\n  (\n        "AWS_ENDPOINT" = "AWS_ENDPOINT",\n        "AWS_ACCESS_KEY" = "AWS_TEMP_ACCESS_KEY",\n        "AWS_SECRET_KEY" = "AWS_TEMP_SECRET_KEY",\n        "AWS_TOKEN" = "AWS_TEMP_TOKEN",\n        "AWS_REGION" = "AWS_REGION"\n  )\n'})}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},120852:function(e,n,t){t.d(n,{Z:function(){return r}});let r=t.p+"assets/files/aws_ak_sk-7a2cc3d09728977381b53e321838ccb1.png"},250065:function(e,n,t){t.d(n,{Z:function(){return i},a:function(){return s}});var r=t(667294);let o={},a=r.createContext(o);function s(e){let n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);